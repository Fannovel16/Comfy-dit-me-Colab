{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ui74hMe575zT","executionInfo":{"status":"ok","timestamp":1695370592430,"user_tz":-420,"elapsed":68060,"user":{"displayName":"","userId":""}},"outputId":"0123baa0-1244-4242-f057-fbe1e5ce2442"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2023-09-22T08:16:34+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"]},{"output_type":"stream","name":"stdout","text":["NgrokTunnel: \"https://31a7-35-197-80-176.ngrok-free.app\" -> \"http://localhost:8188\"\n","/content/Comfy-dit-me-Colab\n","** ComfyUI start up time: 2023-09-22 08:16:35.976949\n","\n","Prestartup times for custom nodes:\n","   0.0 seconds: /content/Comfy-dit-me-Colab/custom_nodes/ComfyUI-Manager\n","\n","Traceback (most recent call last):\n","  File \"/content/Comfy-dit-me-Colab/main.py\", line 69, in <module>\n","    import execution\n","  File \"/content/Comfy-dit-me-Colab/execution.py\", line 11, in <module>\n","    import nodes\n","  File \"/content/Comfy-dit-me-Colab/nodes.py\", line 20, in <module>\n","    import comfy.diffusers_load\n","  File \"/content/Comfy-dit-me-Colab/comfy/diffusers_load.py\", line 4, in <module>\n","    import comfy.sd\n","  File \"/content/Comfy-dit-me-Colab/comfy/sd.py\", line 5, in <module>\n","    from comfy import model_management\n","  File \"/content/Comfy-dit-me-Colab/comfy/model_management.py\", line 114, in <module>\n","    total_vram = get_total_memory(get_torch_device()) / (1024 * 1024)\n","  File \"/content/Comfy-dit-me-Colab/comfy/model_management.py\", line 83, in get_torch_device\n","    return torch.device(torch.cuda.current_device())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 674, in current_device\n","    _lazy_init()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n","    torch._C._cuda_init()\n","RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"]}],"source":["#@title ## **Start** ðŸ’«\n","%cd /content\n","\n","import atexit, requests, subprocess, time, re, os\n","from random import randint\n","from threading import Timer\n","from queue import Queue\n","from IPython.utils import capture\n","from subprocess import getoutput\n","from urllib.parse import unquote\n","try:\n","  start_colab\n","except:\n","  start_colab = int(time.time())-5\n","\n","# Interactive form variables\n","latest_ui = True #@param{type:\"boolean\"}\n","latest_nodes = False #@param{type:\"boolean\"}\n","verbose = False #@param{type:\"boolean\"}\n","drive_output = False #@param{type:\"boolean\"}\n","args=\"--dont-print-server --preview-method auto --enable-cors-header --use-pytorch-cross-attention\" #@param{type:\"string\"}\n","optional_ngrok_token=\"\" #@param{type:\"string\"}\n","optional_ngrok_region = \"ap\" #@param [\"us\", \"eu\", \"au\", \"ap\", \"sa\", \"jp\", \"in\"]\n","optional_huggingface_token=\"\" #@param{type:\"string\"}\n","custom_url=\"https://huggingface.co/NoCrypt/expermental_models/resolve/main/vanilla.safetensors\" #@param{type:\"string\"}\n","\n","#@markdown &nbsp;  &nbsp; You can list multiple stuff by appending them with `, (comma)` and yes it will be auto detected  OR you can manually specify by using prefixes if something goes wrong\n","\n","\n","# Statics\n","repo=\"/content/Comfy-dit-me-Colab\"\n","destination_dir = \"/content/downloaded/\"\n","models_dir = {\n","  \"model\": os.path.join(repo,\"models/checkpoints\"),\n","  \"vae\": os.path.join(repo,\"models/vae\"),\n","  \"vae_approx\": os.path.join(repo,\"models/vae_approx\"),\n","  \"hypernetwork\": os.path.join(repo,\"models/hypernetworks\"),\n","  \"embedding\": os.path.join(repo,\"models/embeddings\"),\n","  \"lora\": os.path.join(repo,\"models/loras\"),\n","  \"gligen\": os.path.join(repo,\"models/gligen\"),\n","  \"diffuser\": os.path.join(repo,\"models/diffusers\"),\n","  \"controlnet\": os.path.join(repo,\"models/controlnet\"),\n","  \"unet\": os.path.join(repo,\"models/unet\"),\n","  \"config\": os.path.join(repo,\"models/configs\"),\n","  \"style_model\": os.path.join(repo,\"models/style_models\"),\n","  \"upscale_model\": os.path.join(repo,\"models/upscale_models\"),\n","  \"clip_vision\": os.path.join(repo,\"models/clip_vision\"),\n","  \"clip\": os.path.join(repo,\"models/clip\"),\n","}\n","token = optional_huggingface_token if optional_huggingface_token else \"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\"\n","user_header = f\"\\\"Authorization: Bearer {token}\\\"\"\n","\n","# Install\n","if not os.path.exists(os.path.join(repo,\"main.py\")):\n","  with capture.capture_output() as cap:\n","    !wget https://huggingface.co/NoCrypt/fast-repo/resolve/main/ubuntu_deps.zip ; unzip ubuntu_deps.zip -d ./deps ; dpkg -i ./deps/* ; rm -rf ubuntu_deps.zip /content/deps/\n","    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /content/cloudflared-linux-amd64 && chmod 777 /content/cloudflared-linux-amd64\n","    !echo -e \"https://huggingface.co/NoCrypt/fast-repo/resolve/main/dep_slick.tar.lz4\\n\\tout=dep_slick.tar.lz4\\nhttps://github.com/Fannovel16/Comfy-dit-me-Colab/releases/download/Colab-hack/content.tar.lz4\\n\\tout=content.tar.lz4\\n\" | aria2c -i- -j5 -x16 -s16 -k1M -c\n","    !tar -xI lz4 -f dep_slick.tar.lz4 --overwrite-dir --directory=/usr/local/lib/python3.10/dist-packages/ #(manual dir)\n","\n","    !tar -xI lz4 -f content.tar.lz4 --directory=/ #/content/sdw/ (auto dir)\n","    !echo -n {start_colab} > {repo}/colabTimer.txt\n","\n","    os.environ[\"SAFETENSORS_FAST_GPU\"]='1'\n","    os.environ[\"CUDA_MODULE_LOADING\"]=\"LAZY\"\n","    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","    os.environ[\"TCMALLOC_AGGRESSIVE_DECOMMIT\"] = \"t\"\n","    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"garbage_collection_threshold:0.9,max_split_size_mb:512\"\n","\n","    !wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4 -c\n","    %env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n","\n","    # !wget https://gist.github.com/NoCrypt/78151be43a8645f3266393602a7c2a61/raw/8026ec49fa8e897d142a26944bb9db3af76856a4/fast_ext_load.patch  -O /content/fast_ext_load.patch -c\n","\n","    %cd {repo}\n","\n","    # !git apply /content/fast_ext_load.patch\n","\n","\n","if verbose:\n","  if latest_ui:\n","    !git pull\n","\n","  if latest_nodes:\n","    !{'for dir in /content/Comfy-dit-me-Colab/custom_nodes/*/; do cd \"$dir\" && git restore . && git fetch origin && git pull; done'}\n","else:\n","  with capture.capture_output() as cap:\n","    if latest_ui:\n","      !git pull\n","\n","    if latest_nodes:\n","      !{'for dir in /content/Comfy-dit-me-Colab/custom_nodes/*/; do cd \"$dir\" && git restore . && git fetch origin && git pull; done'}\n","\n","if drive_output:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    !mkdir -p /content/gdrive/MyDrive/comfyui_output\n","    args += \" --output-directory /content/gdrive/MyDrive/comfyui_output\"\n","\n","def funny_downloader():\n","  def manual_download(url, dst):\n","    url = url[url.find(':')+1:]\n","    if \".json\" in url or \".csv\" in url:\n","      !wget \"{url}\" -O {dst} -c\n","    elif '.yaml' in url or '.yml' in url or 'discord' in url:\n","      !wget \"{url}\" -P {dst} -c\n","    elif 'drive.google' in url:\n","      if 'folders' in url:\n","        !gdown --folder \"{url}\" -O {dst} --fuzzy -c\n","      else:\n","        !gdown \"{url}\" -O {dst} --fuzzy -c\n","    elif 'huggingface' in url:\n","      if '/blob/' in url:\n","        url = url.replace('/blob/', '/resolve/')\n","      parsed_link = '\\n{}\\n\\tout={}'.format(url,unquote(url.split('/')[-1]))\n","      !echo -e \"{parsed_link}\" | aria2c --header={user_header} --console-log-level=error --summary-interval=10 -i- -j5 -x16 -s16 -k1M -c -d \"{dst}\"\n","    elif 'http' in url or 'magnet' in url:\n","      parsed_link = '\"{}\"'.format(url)\n","      !aria2c --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -j5 -x16 -s16 -k1M -c -d {dst} -Z {parsed_link}\n","\n","  def handle_manual(url):\n","    dst = next((value for key, value in models_dir.items() if key in url.split(':')[0]), None)\n","    manual_download(url, dst)\n","\n","  def download(url):\n","    try:\n","      have_drive_link\n","    except:\n","      if \"drive.google.com\" in url:\n","        # I'm sorry drive ID enjoyer, this will make ID useless :(\n","        !pip install -U gdown\n","        have_drive_link = True\n","    links_and_paths = url.split(',')\n","    http_links = []\n","    huggingface_links = []\n","    for link_or_path in links_and_paths:\n","      link_or_path = link_or_path.strip()\n","      if not link_or_path:\n","        continue\n","      if any(link_or_path.startswith(prefix.lower()+\":\") for prefix in models_dir.keys()):\n","        handle_manual(link_or_path)\n","        continue\n","      if '.yaml' in link_or_path or '.yml' in link_or_path or 'discord' in link_or_path or 'drive.google' in link_or_path:\n","        manual_download(link_or_path, destination_dir)\n","      elif 'huggingface' in link_or_path:\n","        if '/blob/' in link_or_path:\n","          link_or_path = link_or_path.replace('/blob/', '/resolve/')\n","        huggingface_links.append(link_or_path)\n","      elif 'http' in link_or_path or 'magnet' in link_or_path:\n","        http_links.append(link_or_path)\n","      elif '/' in link_or_path or \"/content/\" in link_or_path:\n","        if not os.path.exists('/content/gdrive/MyDrive'):\n","          print('Looks like there\\'s a path in your url. You need to mount your drive first.')\n","          from google.colab import drive\n","          drive.mount('/content/gdrive')\n","        gdrive = \"/content/gdrive/MyDrive/\"+link_or_path if \"/content/\" not in link_or_path else link_or_path\n","        !rsync -avr --progress {gdrive} {destination_dir}\n","      else:\n","        !gdown {link_or_path} -O {destination_dir} --fuzzy -c\n","    if http_links:\n","      links_string = ' '.join(['\"{}\"'.format(x) for x in http_links])\n","      !aria2c --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -j5 -x16 -s16 -k1M -c -d {destination_dir}  -Z {links_string}\n","      del links_string\n","    if huggingface_links:\n","      links_string = '\\n'.join(['{}\\n\\tout={}'.format(x,unquote(x.split('/')[-1])) for x in huggingface_links])\n","      !echo -e \"{links_string}\" | aria2c --header={user_header} --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -i- -j5 -x16 -s16 -k1M -c -d {destination_dir}\n","\n","\n","  if verbose:\n","    download(custom_url)\n","  else:\n","    with capture.capture_output() as cap:\n","      download(custom_url)\n","      del cap\n","  # Link all files by filtering accoridng to their type\n","  with capture.capture_output() as cap:\n","    files = [os.path.join(dp,f) for dp, dn, fn in os.walk(destination_dir) for f in fn] # Thanks Aojiru!\n","    for file in files:\n","      name, file_extension = os.path.splitext(file)\n","      if '.aria2' in file:\n","        continue\n","      file_path = os.path.join(destination_dir, file)\n","      file_size = os.path.getsize(file_path)\n","      if \"control_\" in name or \"t2iadapter_\" in name or file_extension == \".pth\":\n","        !ln \"{file_path}\" {models_dir[\"controlnet\"]}\n","      elif file_extension in ['.yaml', '.yml']:\n","        !ln \"{file_path}\" {models_dir[\"config\"]}\n","      elif file_size > 1_500_000_000:\n","        !ln \"{file_path}\" {models_dir[\"model\"]}\n","      elif \"kl-f8\" in name or \"vae_\" in file or \"vae.\" in file or \"vae-\" in file or file_size > 380_000_000:\n","        !ln \"{file_path}\" {models_dir[\"vae\"]}\n","      elif getoutput('if rg -q -o \"lora_unet\" \"'+file_path+'\"; then echo 1; else echo 0; fi') == \"1\":\n","        !ln \"{file_path}\" {models_dir[\"lora\"]}\n","      elif (file_extension == '.pt' or file_extension == '.safetensors') and file_size < 10_000_000:\n","        !ln \"{file_path}\" {models_dir[\"embedding\"]}\n","      else:\n","        !ln \"{file_path}\" {models_dir[\"hypernetwork\"]}\n","    del cap\n","funny_downloader() # dont ask why I made this one function, (for funny collapse)\n","\n","if not optional_ngrok_token:\n","  # CF tunnel by camenduru\n","  def cloudflared(port, metrics_port, output_queue):\n","      atexit.register(lambda p: p.terminate(), subprocess.Popen(['/content/cloudflared-linux-amd64', 'tunnel', '--url', f'http://127.0.0.1:{port}', '--metrics', f'127.0.0.1:{metrics_port}'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT))\n","      attempts, tunnel_url = 0, None\n","      while attempts < 10 and not tunnel_url:\n","          attempts += 1\n","          time.sleep(3)\n","          try:\n","              tunnel_url = re.search(\"(?P<url>https?:\\/\\/[^\\s]+.trycloudflare.com)\", requests.get(f'http://127.0.0.1:{metrics_port}/metrics').text).group(\"url\")\n","          except:\n","              pass\n","      if not tunnel_url:\n","          raise Exception(\"Can't connect to Cloudflare Edge\")\n","      output_queue.put(tunnel_url)\n","  output_queue, metrics_port = Queue(), randint(8100, 9000)\n","  thread = Timer(2, cloudflared, args=(8188, metrics_port, output_queue))\n","  thread.start()\n","  thread.join()\n","  tunnel_url = output_queue.get()\n","  os.environ['webui_url'] = tunnel_url\n","  print(tunnel_url)\n","else:\n","  try:\n","    from pyngrok import conf,ngrok\n","  except:\n","    with capture.capture_output() as _:\n","      !pip install -qqqq --upgrade setuptools\n","      !pip install -qqqq -U pyngrok\n","    from pyngrok import conf,ngrok\n","\n","  def ngrok_tunnel(port,queue,auth_token,region=\"jp\"):\n","    tunnels = ngrok.get_tunnels()\n","    if tunnels : ngrok.kill()\n","    conf.get_default().region = region\n","    ngrok.set_auth_token(auth_token)\n","    url = ngrok.connect(port)\n","    queue.put(url)\n","\n","  ngrok_output_queue = Queue()\n","  ngrok_thread = Timer(2, ngrok_tunnel, args=(8188, ngrok_output_queue, optional_ngrok_token, optional_ngrok_region))\n","  ngrok_thread.start()\n","  ngrok_thread.join()\n","  print(ngrok_output_queue.get())\n","\n","\n","with capture.capture_output() as cap:\n","  !find /content -name \".ipynb_checkpoints\" -type d -exec rm -r {} \\;\n","\n","%cd {repo}\n","!python main.py {args}"]},{"cell_type":"markdown","metadata":{"id":"6Zl8EIzV75zW"},"source":["## **List of prefixes**\n"]},{"cell_type":"markdown","metadata":{"id":"MTNjl40275zY"},"source":["\n","Prefix             | Path\n","-------------------|------------------\n","model | models/checkpoints\n","vae | models/vae\n","vaes_approx | models/vae_approx\n","hypernetwork | models/hypernetworks\n","embedding | models/embeddings\n","lora | models/loras\n","gligen | models/gligen\n","diffuser | models/diffusers\n","controlnet | models/controlnet\n","unet | models/unet\n","config | models/configs\n","style_model | models/style_models\n","upscale_model | models/upscale_models\n","clip_vision | models/clip_vision\n","clip | models/clip\n","\n","<br/>\n","\n","How to use?\n","\n","Example: `vaes:https://huggingface.co/NoCrypt/resources/resolve/main/VAE/wd.vae.safetensors, model:https://huggingface.co/NoCrypt/expermental_models/resolve/main/vanilla.safetensors`\n"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"https://github.com/Fannovel16/Comfy-dit-me-Colab/blob/master/notebooks/slick_comfyui.ipynb","timestamp":1695370651630}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}